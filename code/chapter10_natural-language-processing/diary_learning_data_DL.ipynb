{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 10.7 文本情感分类：使用循环神经网络"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "1.3.1 cuda\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import MeCab\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"    # 服务器中有多个GPU，选择特定的GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATA_ROOT = \"C:\\\\Users\\\\CK\\\\Dive-into-DL-PyTorch\\\\data\"\n",
    "\n",
    "print(torch.__version__, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10.7.1 文本情感分类数据\n",
    "### 10.7.1.1 读取数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "fname = os.path.join(DATA_ROOT, \"chap5.zip\")\n",
    "if not os.path.exists(os.path.join(DATA_ROOT, \"diary_learning_data_DL\")):\n",
    "    print(\"从压缩包解压...\")\n",
    "    with tarfile.open(fname, 'r') as f:\n",
    "        f.extractall(DATA_ROOT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "\n",
      "  0%|          | 0/317 [00:00<?, ?it/s]\n",
      "100%|██████████| 317/317 [00:00<00:00, 2456.94it/s]\n",
      "\n",
      "100%|██████████| 316/316 [00:00<00:00, 3397.09it/s]\n",
      "\n",
      "  0%|          | 0/317 [00:00<?, ?it/s]\n",
      "100%|██████████| 317/317 [00:00<00:00, 1886.48it/s]\n",
      "\n",
      "  0%|          | 0/317 [00:00<?, ?it/s]\n",
      "100%|██████████| 317/317 [00:00<00:00, 2934.59it/s]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def read_diary(folder='train', data_root=\"C:\\\\Users\\\\CK\\\\Dive-into-DL-PyTorch\\\\data\\\\diary_learning_data_DL\"):  # 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_root, folder, label)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
    "                data.append([review, 1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "train_data, test_data = read_diary('train'), read_diary('test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.7.1.2 预处理数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "def get_tokenized_diary(data):  \n",
    "    \"\"\"\n",
    "    data: list of [string, label]\n",
    "    \"\"\"\n",
    "    tagger = MeCab.Tagger('-Owakati')\n",
    "    \n",
    "    def make_wakati(sentence):\n",
    "        # MeCabで分かち書き\n",
    "        sentence = tagger.parse(sentence)\n",
    "        # 半角全角英数字除去\n",
    "        sentence = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", sentence)\n",
    "        # 記号もろもろ除去\n",
    "        sentence = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", sentence)\n",
    "        # スペースで区切って形態素の配列へ\n",
    "        wakati = sentence.split(\" \")\n",
    "        # 空の要素は削除\n",
    "        wakati = list(filter((\"\").__ne__, wakati))\n",
    "        return wakati\n",
    "            \n",
    "    def tokenizer(text):\n",
    "        wakati = make_wakati(text)\n",
    "        return [word for word in wakati]\n",
    "    \n",
    "    return [tokenizer(review) for review, _ in data]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "data": {
      "text/plain": "('# words in vocab:', 1497)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 142
    }
   ],
   "source": [
    "def get_vocab_diary(data):  # 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "    tokenized_data = get_tokenized_diary(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter)\n",
    "\n",
    "vocab = get_vocab_diary(train_data)\n",
    "'# words in vocab:', len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "def preprocess_diary(data, vocab):  # 本函数已保存在d2lzh_torch包中方便以后使用\n",
    "    max_l = 25  # 将每条评论通过截断或者补0，使得长度变成500\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "    tokenized_data = get_tokenized_diary(data)\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([score for _, score in data])\n",
    "    return features, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.7.1.3 创建数据迭代器"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_set = Data.TensorDataset(*preprocess_diary(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*preprocess_diary(test_data, vocab))\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "X torch.Size([32, 25]) y torch.Size([32])\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "('#batches:', 20)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 145
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'#batches:', len(train_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10.7.2 使用循环神经网络的模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        \n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, \n",
    "                                hidden_size=num_hiddens, \n",
    "                                num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 2) # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是(批量大小, 词数)，因为LSTM需要将序列长度(seq_len)作为第一维，所以将输入转置后\n",
    "        # 再提取词特征，输出形状为(词数, 批量大小, 词向量维度)\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))\n",
    "        # rnn.LSTM只传入输入embeddings，因此只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "        # outputs形状是(词数, 批量大小, 2 * 隐藏单元个数)\n",
    "        outputs, _ = self.encoder(embeddings) # output, (h, c)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为\n",
    "        # (批量大小, 4 * 隐藏单元个数)。\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.7.2.1 加载预训练的词向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "jawiki_vocab = Vocab.Vectors(name='C:\\\\Users\\\\CK\\\\Dive-into-DL-PyTorch\\\\data\\\\jawiki_20180420_100d.txt', cache=os.path.join(DATA_ROOT, \"jawiki_20180420_100d\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 0\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\")\n",
    "    return embed\n",
    "\n",
    "net.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, jawiki_vocab))\n",
    "net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.7.2.2 训练并评价模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.7178, train acc 0.490, test acc 0.508, time 0.7 sec\n",
      "epoch 2, loss 0.3049, train acc 0.675, test acc 0.647, time 0.7 sec\n",
      "epoch 3, loss 0.1560, train acc 0.773, test acc 0.741, time 0.8 sec\n",
      "epoch 4, loss 0.0880, train acc 0.850, test acc 0.735, time 0.8 sec\n",
      "epoch 5, loss 0.0526, train acc 0.896, test acc 0.768, time 0.7 sec\n",
      "epoch 6, loss 0.0386, train acc 0.907, test acc 0.738, time 0.7 sec\n",
      "epoch 7, loss 0.0234, train acc 0.938, test acc 0.749, time 0.7 sec\n",
      "epoch 8, loss 0.0182, train acc 0.942, test acc 0.762, time 0.7 sec\n",
      "epoch 9, loss 0.0108, train acc 0.961, test acc 0.778, time 0.7 sec\n",
      "epoch 10, loss 0.0052, train acc 0.983, test acc 0.756, time 0.7 sec\n",
      "epoch 11, loss 0.0057, train acc 0.983, test acc 0.748, time 0.6 sec\n",
      "epoch 12, loss 0.0043, train acc 0.978, test acc 0.782, time 0.7 sec\n",
      "epoch 13, loss 0.0013, train acc 0.992, test acc 0.795, time 0.6 sec\n",
      "epoch 14, loss 0.0146, train acc 0.935, test acc 0.711, time 0.8 sec\n",
      "epoch 15, loss 0.0085, train acc 0.943, test acc 0.749, time 0.7 sec\n",
      "epoch 16, loss 0.0024, train acc 0.987, test acc 0.779, time 0.7 sec\n",
      "epoch 17, loss 0.0013, train acc 0.994, test acc 0.782, time 0.8 sec\n",
      "epoch 18, loss 0.0003, train acc 0.998, test acc 0.795, time 0.7 sec\n",
      "epoch 19, loss 0.0002, train acc 0.998, test acc 0.782, time 0.6 sec\n",
      "epoch 20, loss 0.0003, train acc 0.998, test acc 0.790, time 0.7 sec\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 20\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "d2l.train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh包中方便以后使用\n",
    "def predict_sentiment(net, vocab, sentence):\n",
    "    \"\"\"sentence是词语的列表\"\"\"\n",
    "    device = list(net.parameters())[0].device\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    return 'positive' if label.item() == 1 else 'negative'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "'negative'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 152
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['猫', 'が', '脱走', 'し', 'て', 'しまっ', 'た', 'が', '帰っ', 'て', 'き', 'た'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "'negative'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 153
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['ふと', '気', 'が', '付い', 'たら', '真夜中', 'に', 'なっ', 'て', 'い', 'て', '明日', '朝', '早い', 'ので', '後悔', 'し', 'た'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.16.5 $K$折交叉验证"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    # 返回第i折交叉验证时所需要的训练和验证数据\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat((X_train, X_part), dim=0)\n",
    "            y_train = torch.cat((y_train, y_part), dim=0)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def k_fold(k, X_train, y_train, num_epochs,\n",
    "           learning_rate, weight_decay, batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net(X_train.shape[1])\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
    "                                   weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "        if i == 0:\n",
    "            d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse',\n",
    "                         range(1, num_epochs + 1), valid_ls,\n",
    "                         ['train', 'valid'])\n",
    "        print('fold %d, train rmse %f, valid rmse %f' % (i, train_ls[-1], valid_ls[-1]))\n",
    "    return train_l_sum / k, valid_l_sum / k"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.16.6 模型选择"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k, num_epochs, lr, weight_decay, batch_size = 10, 100, 5, 0, 64\n",
    "train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)\n",
    "print('%d-fold validation: avg train rmse %f, avg valid rmse %f' % (k, train_l, valid_l))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}